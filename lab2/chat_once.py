from dotenv import load_dotenv
from google import genai
from google.genai import types
import os, torch, time
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from openai import OpenAI

load_dotenv()

MODEL_MODE = os.getenv("MODEL_MODE", 'groq')

LOCAL_MODEL_NAME = os.getenv("LOCAL_MODEL_NAME", "Qwen/Qwen2.5-1.5B-Instruct")

GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", None)

BASE_URL = os.getenv("GROQ_BASE_URL")
MODEL_NAME = os.getenv("GROQ_MODEL_NAME")
GROQ_API_KEY = os.getenv("GROQ_API_KEY", None)

SYSTEM_PROMPT = "Odpowiadaj po polsku i zwięźle."
USER_PROMPT = "Podaj 3 krótkie pomysły na szybkie dania."

tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_NAME,
    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None,
)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

def build_prompt(system: str, user: str) -> str:
    messages = [{"role": "system", "content": system}, {"role": "user", "content": user}]
    if hasattr(tokenizer, "apply_chat_template"):
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    return f"[SYSTEM]\n{system}\n[USER]\n{user}\n[ASSISTANT]\n"


def count_tokens_local(text: str) -> int:
    return len(tokenizer.encode(text))

@torch.inference_mode()
def local_generate(
        prompt: str,
        system: str = "You are a helpful assistant.",
        max_output_tokens: int = 128,
        temperature: float = 0.0,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
) -> Dict[str, Any]:
    t0 = time.perf_counter()
    text = build_prompt(system, prompt)
    inputs = tokenizer(text, return_tensors="pt").to(device)
    do_sample = temperature > 0.0
    gen_kwargs: Dict[str, Any] = dict(
        max_new_tokens=max_output_tokens,
        do_sample=do_sample,
        eos_token_id=tokenizer.eos_token_id,
    )
    if do_sample:
        gen_kwargs.update(dict(temperature=temperature, top_p=top_p))
        if top_k is not None:
            gen_kwargs["top_k"] = int(top_k)
    output_ids = model.generate(**inputs, **gen_kwargs)
    gen_only = output_ids[0, inputs["input_ids"].shape[-1]:]
    output_txt = tokenizer.decode(gen_only, skip_special_tokens=True)
    dt = time.perf_counter() - t0
    ptoks = count_tokens_local(prompt) + count_tokens_local(system)
    ctoks = count_tokens_local(output_txt)
    return {
        "text": output_txt,
        "latency_s": round(dt, 3),
        "usage": {
            "prompt_tokens": ptoks,
            "completion_tokens": ctoks,
            "total_tokens": ptoks + ctoks,
        }
    }


def gemini_generate(prompt : str, system : str = "You are a helpful assistant", temperature : float = 0.0, top_p : float = 1.0, top_k : int = 40, max_output_tokens : int = 256) -> dict:
    gclient = genai.Client(api_key=GOOGLE_API_KEY)
    t0 = time.perf_counter()
    response = gclient.models.generate_content(
        model=GEMINI_MODEL,
        contents=prompt,
        config=types.GenerateContentConfig(
            system_instruction=system,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            max_output_tokens=max_output_tokens
        ),
    )
    dt = time.perf_counter() - t0
    usage = getattr(response, "usage_metadata", None)
    usage_dict = {
        "prompt_tokens": getattr(usage, "prompt_token_count", None),
        "completion_tokens": getattr(usage, "candidates_token_count", None),
        "total_tokens": getattr(usage, "total_token_count", None),
    } if usage is not None else None
    text = getattr(response, "text", None)
    gclient.close()
    return {
        "text": text if text is not None else str(response),
        "latency_s": round(dt, 3),
        "usage": usage_dict,
    }

def groq_generate(prompt : str, system : str = "You are a helpful assistant", temperature : float = 0.0, top_p : float = 1.0, max_output_tokens : int = 256) -> dict:
    client = OpenAI(api_key=GROQ_API_KEY, base_url=BASE_URL)
    t0 = time.perf_counter()
    response = client.responses.create(
        model=MODEL_NAME,
        instructions=system,
        input=prompt,
        temperature=temperature,
        top_p=top_p,
        max_output_tokens=max_output_tokens,
    )
    dt = time.perf_counter() - t0
    #usage - informacja o zużyciu zasobów (tokenów) przez model podczas generowania odpowiedzi
    usage = getattr(response, "usage", None)
    usage_dict = None if usage is None else getattr(usage, "model_dump", lambda: usage)()
    client.close()
    return {
        "text": response.output_text,
        "latency_s": round(dt, 3),
        "usage": usage_dict,
    }


def chat_once(prompt : str, system : str = "You are a helpful assistant", temperature : float = 0.0, top_p : float = 1.0, top_k : Optional[int] = None, max_output_tokens : int = 256) -> dict:
    if MODEL_MODE == "gemini":
        return gemini_generate(
            prompt=prompt,
            system=system,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
        )
    elif MODEL_MODE == "groq":
        return groq_generate(
            prompt=prompt,
            system=system,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
            top_p=top_p,
        )
    else:
        return local_generate(
            prompt=prompt,
            system=system,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
        )